{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXhpw+HQP4XfP6BgzWjcPw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aashu-0/learn-pytorch/blob/main/05_modular_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMziuTXdxgr5",
        "outputId": "b58de111-2666-4429-eab2-e39b8e4c4c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a directory `going_modular` and store all `.py` scripts there"
      ],
      "metadata": {
        "id": "ww7svnhscJYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('going_modular', exist_ok=True)"
      ],
      "metadata": {
        "id": "Ea530w5hcWID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing `get_data.py` for Data Downloading"
      ],
      "metadata": {
        "id": "9a9ZKY5Oa_g8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nKNYVcKYo6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5376b50c-d210-4289-b66d-85ced3dbdefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/get_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile going_modular/get_data.py\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# setup path\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# download if, image folder doesn't exists\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory already exists...skipping download\")\n",
        "else:\n",
        "  print(f\"{image_path} does not exists...creating one\")\n",
        "  image_path.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "\n",
        "# download zip file from daniel github\n",
        "with open(data_path/ \"pizza_steak_sushi.zip\", 'wb') as f:\n",
        "  request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  print('Downloading....the github zip file')\n",
        "  f.write(request.content)\n",
        "\n",
        "# unzip the data\n",
        "with zipfile.ZipFile(data_path/'pizza_steak_sushi.zip', 'r') as zip_ref:\n",
        "  print('Unzipping the zip file')\n",
        "  zip_ref.extractall(image_path)\n",
        "\n",
        "# remove the zip file\n",
        "os.remove(data_path/'pizza_steak_sushi.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing `data_setup.py` file to create Dataset and DataLoaders"
      ],
      "metadata": {
        "id": "BQHPFwr1dadn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(train_dir: str,\n",
        "                       test_dir: str,\n",
        "                       transform: transforms.Compose,\n",
        "                       batch_size: int,\n",
        "                       num_workers: NUM_WORKERS):\n",
        "\n",
        "  # load image data using ImageFolder\n",
        "  train_data = datasets.ImageFolder(train_dir,\n",
        "                                  transform= transform,)\n",
        "\n",
        "  test_data = datasets.ImageFolder(root=test_dir,\n",
        "                                 transform= transform)\n",
        "\n",
        "  # get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  #turn image dataset into dataloaders\n",
        "  train_dataloader = DataLoader(train_data,\n",
        "                               batch_size = batch_size,\n",
        "                               num_workers= NUM_WORKERS,\n",
        "                               shuffle = True)\n",
        "  test_dataloader = DataLoader(test_data,\n",
        "                              batch_size = batch_size,\n",
        "                              shuffle = False,\n",
        "                               num_workers= NUM_WORKERS)\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-aCrskcbreF",
        "outputId": "01dce650-b315-4d56-814d-b88c5bedaf52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Writing `model_builder.py` to create Model"
      ],
      "metadata": {
        "id": "nHaWV7ung8zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int)-> None:\n",
        "    super().__init__()\n",
        "    self.conv_block1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2) # by default equal to kernel_size\n",
        "    )\n",
        "    self.conv_block2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2) # by default equal to kernel_size\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*13*13,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block1(x)\n",
        "    x = self.conv_block2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "    # return self.classifier(self.conv_block2(self.conv_block1(x))) # <--using operation fusion we can do all above in single step\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En_pgG2ug4LT",
        "outputId": "dea2cccb-d137-4604-85a7-88fb55bb44b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing `engine.py`\n",
        "to put `train_step()` , `test_step()` and `train()` functions together"
      ],
      "metadata": {
        "id": "xV77dIOdi2hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\n",
        "import torchmetrics\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "\n",
        "# train step func\n",
        "def train_step(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    accuracy_fn: torchmetrics.Accuracy,\n",
        "    device: torch.device)-> Tuple[float, float]:\n",
        "\n",
        "  # to train mode\n",
        "  model.train()\n",
        "\n",
        "  train_loss, train_acc = 0,0\n",
        "\n",
        "  # loop through each batch\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    #.to(device)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # do the forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loss backward (backprop)\n",
        "    loss.backward()\n",
        "\n",
        "    # optimizer step (grad descent)\n",
        "    optimizer.step()\n",
        "\n",
        "    # accuracy\n",
        "    train_acc += accuracy_fn(y_pred, y)\n",
        "\n",
        "\n",
        "  # avg per batch\n",
        "  train_loss /=len(dataloader)\n",
        "  train_acc /= len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "# test step func\n",
        "def test_step(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    loss_fn: torch.nn.Module,\n",
        "    accuracy_fn: torchmetrics.Accuracy,\n",
        "    device: torch.device) -> Tuple[float, float]:\n",
        "\n",
        "  # to eval mode\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, test_acc = 0,0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "      # to.device\n",
        "      X,y = X.to(device), y.to(device)\n",
        "\n",
        "      # do forward pass -> raw logits\n",
        "      test_pred_logits = model(X)\n",
        "\n",
        "      # calculate the loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # accuracy\n",
        "      test_acc += accuracy_fn(test_pred_logits, y)\n",
        "\n",
        "  # avg\n",
        "  test_loss = test_loss/ len(dataloader)\n",
        "  test_acc = test_acc/ len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "\n",
        "#train func\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          accuracy_fn: torchmetrics.Accuracy,\n",
        "          epochs: int,\n",
        "          device= torch.device)-> dict[str, List]:\n",
        "\n",
        "  # empty result dict\n",
        "  results = {'train_loss': [],\n",
        "             'train_acc': [],\n",
        "             'test_loss': [],\n",
        "             'test_acc': []}\n",
        "  # loop\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "                                       dataloader = train_dataloader,\n",
        "                                       loss_fn = loss_fn,\n",
        "                                       optimizer= optimizer,\n",
        "                                       accuracy_fn=accuracy_fn,\n",
        "                                       device = device)\n",
        "\n",
        "    test_loss, test_acc = test_step(model = model,\n",
        "                                    dataloader = test_dataloader,\n",
        "                                    loss_fn = loss_fn,\n",
        "                                    accuracy_fn=accuracy_fn,\n",
        "                                    device = device)\n",
        "    # print out what's happening\n",
        "    print(f'Epoch: {epoch} | Train Loss: {train_loss:.4f} | Train acc: {train_acc:.4f} | Test Loss: {test_loss:.4f} | Test acc: {test_acc:.4f}')\n",
        "\n",
        "    # update the dict\n",
        "    results['train_loss'].append(train_loss)\n",
        "    results['train_acc'].append(train_acc)\n",
        "    results['test_loss'].append(test_loss)\n",
        "    results['test_acc'].append(test_acc)\n",
        "\n",
        "  #return the end results\n",
        "  return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evzc1T3siwiB",
        "outputId": "8b631593-8b20-434b-b96c-fdc32ca41817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Writing `utils.py` to save the model"
      ],
      "metadata": {
        "id": "XLhMDMuAm9bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# func to save the model after training\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "\n",
        "  # create target dir\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok= True)\n",
        "\n",
        "  # create model save path\n",
        "  assert model_name.endswith('.pth') or model_name.endswith('pt')\n",
        "  model_save_path = target_dir_path /model_name\n",
        "\n",
        "  # save model state_dict()\n",
        "  print(f'Saving model to: {model_save_path}')\n",
        "  torch.save(obj= model.state_dict(),\n",
        "             f = model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYmf7A0tm6UV",
        "outputId": "7ff9b5d9-c7e0-4eb1-f038-68326b6cf573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Writing `train.py` to train, evaluate and save the model\n",
        "\n",
        "- combining all the functionality of all the other python scripts\n",
        "- so that we can train a model using a single line of code\n",
        "```\n",
        "python train.py\n",
        "```\n",
        "1. import all the dependencies\n",
        "2. import other modules in `going_modular` directory\n",
        "3. setup hyperparams\n",
        "4. train and test fun\n",
        "5. device-agnostic code\n",
        "6. data transforms\n",
        "7. dataloaders\n",
        "8. create model\n",
        "9. setup loss and optimizer\n",
        "10. train the model\n",
        "11. save the model\n"
      ],
      "metadata": {
        "id": "MPx6UbkLoxlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "import os\n",
        "import torch\n",
        "import torchmetrics\n",
        "import get_data, data_setup, engine, model_builder, utils\n",
        "from torchvision import transforms\n",
        "\n",
        "# hyperparams\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "HIDDEN_UNITS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_WORKERS= os.cpu_count()\n",
        "\n",
        "# directories\n",
        "train_dir = get_data.image_path/'train'\n",
        "test_dir = get_data.image_path/'test'\n",
        "\n",
        "# device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# transforms\n",
        "data_transform = transforms.Compose([transforms.Resize((64,64)),\n",
        "                                     transforms.ToTensor()])\n",
        "\n",
        "# dataloaders from data_setup.py\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir = train_dir,\n",
        "    test_dir = test_dir,\n",
        "    transform = data_transform,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    num_workers = NUM_WORKERS\n",
        ")\n",
        "\n",
        "# model from mode_builder.py\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units= HIDDEN_UNITS,\n",
        "    output_shape = len(class_names)\n",
        ").to(device)\n",
        "\n",
        "# loss, optimizer and accuracy\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "accuracy_fn = torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr= LEARNING_RATE)\n",
        "\n",
        "# training using engine.py\n",
        "engine.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             accuracy_fn= accuracy_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs = NUM_EPOCHS,\n",
        "             device = device)\n",
        "\n",
        "# save model using utils.py\n",
        "utils.save_model(model=model,\n",
        "                 target_dir='models',\n",
        "                 model_name='test_modular_tinyvgg.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GImt8mX-olPJ",
        "outputId": "3e9d2dac-52a4-4f3c-df25-05e9d45bfac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's train out model"
      ],
      "metadata": {
        "id": "3fhGIHERwdSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIj5KmqCwZpe",
        "outputId": "28a54e7b-049f-4f26-a7a7-6edd4054b4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pizza_steak_sushi does not exists...creating one\n",
            "Downloading....the github zip file\n",
            "Unzipping the zip file\n",
            "  0% 0/5 [00:00<?, ?it/s]Epoch: 0 | Train Loss: 1.0962 | Train acc: 0.3867 | Test Loss: 1.1108 | Test acc: 0.2604\n",
            " 20% 1/5 [00:02<00:08,  2.09s/it]Epoch: 1 | Train Loss: 1.1073 | Train acc: 0.3047 | Test Loss: 1.1120 | Test acc: 0.2604\n",
            " 40% 2/5 [00:02<00:04,  1.38s/it]Epoch: 2 | Train Loss: 1.0834 | Train acc: 0.4375 | Test Loss: 1.1242 | Test acc: 0.3125\n",
            " 60% 3/5 [00:03<00:02,  1.16s/it]Epoch: 3 | Train Loss: 1.0663 | Train acc: 0.4883 | Test Loss: 1.1307 | Test acc: 0.3021\n",
            " 80% 4/5 [00:04<00:01,  1.06s/it]Epoch: 4 | Train Loss: 1.0889 | Train acc: 0.4023 | Test Loss: 1.1076 | Test acc: 0.2500\n",
            "100% 5/5 [00:05<00:00,  1.13s/it]\n",
            "Saving model to: models/test_modular_tinyvgg.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UsJYvk_5wkcR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}